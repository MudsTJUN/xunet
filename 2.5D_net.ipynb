{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim.lr_scheduler import StepLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['id', 'caseday', 'imgpath_0', 'imgpath_1', 'imgpath_2', 'imgpath_3',\n",
      "       'imgpath_4', 'imgpath_5', 'imgpath_6', 'imgpath_7', 'labelpath_0',\n",
      "       'labelpath_1', 'labelpath_2', 'labelpath_3', 'labelpath_4',\n",
      "       'labelpath_5', 'labelpath_6', 'labelpath_7', 'height', 'width',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "Index(['id', 'caseday', 'imgpath_0', 'imgpath_1', 'imgpath_2', 'imgpath_3',\n",
      "       'imgpath_4', 'imgpath_5', 'imgpath_6', 'imgpath_7', 'labelpath_0',\n",
      "       'labelpath_1', 'labelpath_2', 'labelpath_3', 'labelpath_4',\n",
      "       'labelpath_5', 'labelpath_6', 'labelpath_7', 'height', 'width',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "Index(['id', 'caseday', 'imgpath_0', 'imgpath_1', 'imgpath_2', 'imgpath_3',\n",
      "       'imgpath_4', 'imgpath_5', 'imgpath_6', 'imgpath_7', 'labelpath_0',\n",
      "       'labelpath_1', 'labelpath_2', 'labelpath_3', 'labelpath_4',\n",
      "       'labelpath_5', 'labelpath_6', 'labelpath_7', 'height', 'width',\n",
      "       'label'],\n",
      "      dtype='object')\n",
      "Index(['id', 'caseday', 'imgpath_0', 'imgpath_1', 'imgpath_2', 'imgpath_3',\n",
      "       'imgpath_4', 'imgpath_5', 'imgpath_6', 'imgpath_7', 'labelpath_0',\n",
      "       'labelpath_1', 'labelpath_2', 'labelpath_3', 'labelpath_4',\n",
      "       'labelpath_5', 'labelpath_6', 'labelpath_7', 'height', 'width',\n",
      "       'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_3D8_df = pd.read_csv(\"./data/train_3D8_df.csv\")\n",
    "val_3D8_df = pd.read_csv(\"./data/val_3D8_df.csv\")\n",
    "test_3D8_df = pd.read_csv(\"./data/test_3D8_df.csv\")\n",
    "\n",
    "train_3D8_df = train_3D8_df.drop(train_3D8_df.columns[[0]], axis=1)\n",
    "val_3D8_df = val_3D8_df.drop(val_3D8_df.columns[[0]], axis=1)\n",
    "test_3D8_df = test_3D8_df.drop(test_3D8_df.columns[[0]], axis=1)\n",
    "\n",
    "train_df = train_3D8_df[train_3D8_df.label == 1].reset_index(drop=True)  #ラベル有のデータのみ抽出\n",
    "val_df = val_3D8_df[val_3D8_df.label == 1].reset_index(drop=True)  #ラベル有のデータのみ抽出\n",
    "test_df = test_3D8_df[test_3D8_df.label == 1].reset_index(drop=True)  #ラベル有のデータのみ抽出\n",
    "\n",
    "print(train_3D8_df.columns)\n",
    "print(val_3D8_df.columns)\n",
    "print(train_df.columns)\n",
    "print(val_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # データローダーの作成\n",
    "# additional_image_targets = {f\"image{i+1}\": \"image\" for i in range(8 - 1)}\n",
    "# additional_label_targets = {f\"mask{i+1}\": \"mask\" for i in range(8 - 1)}\n",
    "# additional_targets8 = dict(additional_image_targets, **additional_label_targets)\n",
    "\n",
    "# data_transforms8 = A.Compose([\n",
    "#       A.HorizontalFlip(p=0.5),\n",
    "#       A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
    "#       A.OneOf([\n",
    "#           A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n",
    "#           A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
    "#       ], p=0.25),],\n",
    "#       additional_targets = additional_targets8, p=1.0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 以下のコードの説明\n",
    "1. 各深さの画像とラベルを読み込み、（256,256）にリサイズした後、深さ方向に結合\n",
    "2. Albumentationsの引数とする為辞書型のデータを作成\n",
    "3. Albumentationsによるデータ拡張\n",
    "4. 変換後のデータをサイド深さ方向に結合\n",
    "5. 画像は各RGBの値を0~1に変換（255で割る）\n",
    "6. ラベルは0（background）、1(large_bowel),2(small_bowel),3(stomach)で表されているため、これをOne -Hotエンコーディング\n",
    "7. 画像・ラベルデータを（バッチサイズ、チャンネル数、深さ、高さ、幅）に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomDataset(torch.utils.data.Dataset):\n",
    "#   def __init__(\n",
    "#       self,\n",
    "#       df\n",
    "#       ):\n",
    "#     self.depth = 8\n",
    "#     self.imgpath_list = [[df.iloc[i,:][f\"imgpath_{h}\"] for h in range(self.depth)] for i in range(len(df))]\n",
    "#     self.labelpath_list = [[df.iloc[i,:][f\"labelpath_{h}\"] for h in range(self.depth)] for i in range(len(df))]\n",
    "#     self.transform8 = data_transforms8\n",
    "\n",
    "#   def __getitem__(self, i):\n",
    "#     imgpathes = self.imgpath_list[i]\n",
    "#     labelpathes = self.labelpath_list[i]\n",
    "#     for j in range(self.depth):  #画像とラベルデータの読み込み\n",
    "#       img = cv2.imread(imgpathes[j])\n",
    "#       img = cv2.resize(img, dsize = (256, 256))\n",
    "#       label = Image.open(labelpathes[j])\n",
    "#       label = np.asarray(label)\n",
    "#       label = cv2.resize(label, dsize = (256, 256))\n",
    "#       if j == 0:  #深さ方向に結合\n",
    "#         img_3D = [img]\n",
    "#         label_3D = [label]\n",
    "#       else:\n",
    "#         img_3D = np.vstack([img_3D, [img]])\n",
    "#         label_3D = np.vstack([label_3D, [label]])\n",
    "      \n",
    "#     d1 = {\"image\": img_3D[0,:,:,:]}  #Albumentationsに代入する為の辞書型データを作成\n",
    "#     d2 = {f\"image{i+1}\": img_3D[i+1,:,:,:] for i in range(self.depth - 1)}\n",
    "#     d3 = {\"mask\": label_3D[0,:,:]}\n",
    "#     d4 = {f\"mask{i+1}\": label_3D[i+1,:,:] for i in range(self.depth - 1)}\n",
    "#     dic = dict(d1, **d2, **d3, **d4)\n",
    "\n",
    "#     transformed = self.transform8(**dic)\n",
    "\n",
    "#     for j in range(self.depth):\n",
    "#       if j == 0:  #データ拡張後のデータを再度深さ方向に結合\n",
    "#         img_3D = [transformed[\"image\"]]\n",
    "#         label_3D = [transformed[\"mask\"]]\n",
    "#       else:\n",
    "#         img_3D = np.vstack([img_3D, [transformed[f\"image{j}\"]]])\n",
    "#         label_3D = np.vstack([label_3D, [transformed[f\"mask{j}\"]]])\n",
    "\n",
    "#     img_3D = img_3D/255  #RGBの値を0~1に\n",
    "#     img_3D = torch.from_numpy(img_3D.astype(np.float32)).clone()\n",
    "#     img_3D = img_3D.permute(3, 0, 1, 2)  #（チャンネル数、深さ、高さ、幅）に変換\n",
    "#     label_3D = torch.from_numpy(label_3D.astype(np.float32)).clone()\n",
    "#     label_3D = torch.nn.functional.one_hot(label_3D.long(), num_classes=4)\n",
    "#     label_3D = label_3D.to(torch.float32)\n",
    "#     label_3D = label_3D.permute(3, 0, 1, 2)  #（チャンネル数、深さ、高さ、幅）に変換\n",
    "#     data = {\"img\": img_3D, \"label\": label_3D}\n",
    "#     return data\n",
    "  \n",
    "#   def __len__(self):\n",
    "#     return len(self.imgpath_list)\n",
    "\n",
    "\n",
    "# class valtest_Dataset(torch.utils.data.Dataset):  #Albumentationsによるデータ拡張を行わない\n",
    "#   def __init__(\n",
    "#       self,\n",
    "#       df\n",
    "#       ):\n",
    "#     self.depth = 8\n",
    "#     self.imgpath_list = [[df.iloc[i,:][f\"imgpath_{h}\"] for h in range(self.depth)] for i in range(len(df))]\n",
    "#     self.labelpath_list = [[df.iloc[i,:][f\"labelpath_{h}\"] for h in range(self.depth)] for i in range(len(df))]\n",
    "\n",
    "#   def __getitem__(self, i):\n",
    "#     imgpathes = self.imgpath_list[i]\n",
    "#     labelpathes = self.labelpath_list[i]\n",
    "#     for j in range(self.depth):\n",
    "#       img = cv2.imread(imgpathes[j])\n",
    "#       label = Image.open(labelpathes[j])\n",
    "#       label = np.asarray(label)\n",
    "#       img = cv2.resize(img, dsize = (256, 256))\n",
    "#       label = cv2.resize(label, dsize = (256, 256))\n",
    "#       if j == 0:\n",
    "#         img_3D = [img]\n",
    "#         label_3D = [label]\n",
    "#       else:\n",
    "#         img_3D = np.vstack([img_3D, [img]])\n",
    "#         label_3D = np.vstack([label_3D, [label]])\n",
    "\n",
    "#     img_3D = img_3D/255\n",
    "#     img_3D = torch.from_numpy(img_3D.astype(np.float32)).clone()\n",
    "#     img_3D = img_3D.permute(3, 0, 1, 2)\n",
    "#     label_3D = torch.from_numpy(label_3D.astype(np.float32)).clone()\n",
    "#     label_3D = torch.nn.functional.one_hot(label_3D.long(), num_classes=4)\n",
    "#     label_3D = label_3D.to(torch.float32)\n",
    "#     label_3D = label_3D.permute(3, 0, 1, 2)\n",
    "#     data = {\"img\": img_3D, \"label\": label_3D}\n",
    "#     return data\n",
    "  \n",
    "#   def __len__(self):\n",
    "#     return len(self.imgpath_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "\n",
    "from boring import CustomDataset, valtest_Dataset\n",
    "    \n",
    "train_dataset = CustomDataset(train_df)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=2,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True,\n",
    "                          drop_last=True)\n",
    "\n",
    "val_dataset = valtest_Dataset(val_df)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=2,\n",
    "                          shuffle=True,\n",
    "                          pin_memory=True)\n",
    "\n",
    "test_dataset = valtest_Dataset(test_df)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=1,\n",
    "                          num_workers=4,\n",
    "                          pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<boring.CustomDataset at 0x29ed6efe0>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataloader.DataLoader at 0x10522f070>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2.5DのUNetモデルを構築\n",
    "class TwoConvBlock_2D(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.rl(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.rl(x)\n",
    "        return x\n",
    "\n",
    "class TwoConvBlock_3D(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, middle_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm3d(middle_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        self.conv2 = nn.Conv3d(middle_channels, out_channels, kernel_size = 3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.rl(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.rl(x)\n",
    "        return x\n",
    "\n",
    "class UpConv_2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size = 2, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "class UpConv_3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"trilinear\", align_corners=True)\n",
    "        self.bn1 = nn.BatchNorm3d(in_channels)\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size = 2, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "\n",
    "class UNet_3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.TCB1 = TwoConvBlock_3D(3, 64, 64)\n",
    "        self.TCB2 = TwoConvBlock_3D(64, 128, 128)\n",
    "        self.TCB3 = TwoConvBlock_3D(128, 256, 256)\n",
    "        self.TCB4 = TwoConvBlock_2D(256, 512, 512)\n",
    "        self.TCB5 = TwoConvBlock_2D(512, 1024, 1024)\n",
    "        self.TCB6 = TwoConvBlock_2D(1024, 512, 512)\n",
    "        self.TCB7 = TwoConvBlock_3D(512, 256, 256)\n",
    "        self.TCB8 = TwoConvBlock_3D(256, 128, 128)\n",
    "        self.TCB9 = TwoConvBlock_3D(128, 64, 64)\n",
    "\n",
    "        self.maxpool_3D = nn.MaxPool3d(2, stride = 2)\n",
    "        self.maxpool_2D = nn.MaxPool2d(2, stride = 2)\n",
    "\n",
    "        self.UC1 = UpConv_2D(1024, 512) \n",
    "        self.UC2 = UpConv_2D(512, 256) \n",
    "        self.UC3 = UpConv_3D(256, 128) \n",
    "        self.UC4 = UpConv_3D(128, 64)\n",
    "\n",
    "        self.conv1 = nn.Conv3d(64, 4, kernel_size = 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.TCB1(x)\n",
    "        x1 = x\n",
    "        x = self.maxpool_3D(x)\n",
    "\n",
    "        x = self.TCB2(x)\n",
    "        x2 = x\n",
    "        x = self.maxpool_3D(x)\n",
    "\n",
    "        x = self.TCB3(x)\n",
    "        x3 = x\n",
    "        x_1, x_2 = x[:,:,0,:,:], x[:,:,1,:,:] \n",
    "        x_1, x_2 = self.maxpool_2D(x_1), self.maxpool_2D(x_2) \n",
    "\n",
    "        x_1, x_2 = self.TCB4(x_1), self.TCB4(x_1)\n",
    "        x4_1, x4_2 = x_1, x_2 \n",
    "        x_1, x_2 = self.maxpool_2D(x_1), self.maxpool_2D(x_2)\n",
    "\n",
    "        x_1, x_2 = self.TCB5(x_1), self.TCB5(x_1)\n",
    "\n",
    "        x_1, x_2 = self.UC1(x_1), self.UC1(x_2)\n",
    "        x_1, x_2 = torch.cat([x4_1, x_1], dim = 1), torch.cat([x4_2, x_2], dim = 1)\n",
    "        x_1, x_2 = self.TCB6(x_1), self.TCB6(x_1)\n",
    "\n",
    "        x_1, x_2 = self.UC2(x_1), self.UC2(x_2)\n",
    "        x = torch.cat([torch.unsqueeze(x_1, 2), torch.unsqueeze(x_2, 2)], dim = 2)\n",
    "        x = torch.cat([x3, x], dim = 1)\n",
    "        x = self.TCB7(x)\n",
    "\n",
    "        x = self.UC3(x)\n",
    "        x = torch.cat([x2, x], dim = 1)\n",
    "        x = self.TCB8(x)\n",
    "\n",
    "        x = self.UC4(x)\n",
    "        x = torch.cat([x1, x], dim = 1)\n",
    "        x = self.TCB9(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU、最適化アルゴリズムの設定\n",
    "device = torch.device('cpu') # M1macbookproの場合はGPUは'mps'にする\n",
    "\n",
    "unet = UNet_3D().to(device)\n",
    "optimizer = optim.Adam(unet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の設定。損失はTversky LossとBCEWithLogits Lossの平均とした。\n",
    "# これらの関数は損失関数内でソフトマックス関数を処理する為、UNetの最後にソフトマックス関数を適用していない\n",
    "TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n",
    "BCELoss     = smp.losses.SoftBCEWithLogitsLoss()\n",
    "def criterion(pred,target):\n",
    "    return 0.5*BCELoss(pred, target) + 0.5*TverskyLoss(pred, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 - Start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check\n",
      "check1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/t-jun/future/.venv/lib/python3.10/site-packages/torch/nn/modules/conv.py:456: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/Convolution.cpp:1009.)\n",
      "  return F.conv2d(input, weight, bias, self.stride,\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m現在のセルまたは前のセルでコードを実行中に、カーネルがクラッシュしました。エラーの原因を特定するには、セル内のコードを確認してください。詳細については、<a href='https://aka.ms/vscodeJupyterKernelCrash'>こちら</a> をクリックしてください。さらなる詳細については、Jupyter [log] (command:jupyter.viewOutput) を参照してください。"
     ]
    }
   ],
   "source": [
    "# 学習を行う\n",
    "history = {\"train_loss\": []}\n",
    "n = 0\n",
    "m = 0\n",
    "# 例: ステップごとに学習率を減衰させる\n",
    "scheduler = StepLR(optimizer, step_size=5, gamma=0.1)\n",
    "\n",
    "for epoch in range(15):\n",
    "  train_loss = 0\n",
    "  val_loss = 0\n",
    "  print(f\"Epoch {epoch + 1} - Start\")\n",
    "\n",
    "  unet.train()\n",
    "  \n",
    "  for i, data in enumerate(train_loader):\n",
    "    print(\"check\")\n",
    "    inputs, labels = data[\"img\"].to(device), data[\"label\"].to(device)\n",
    "    print('check1')\n",
    "    optimizer.zero_grad()\n",
    "    outputs = unet(inputs)\n",
    "    loss = criterion(outputs, labels)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    train_loss += loss.item()\n",
    "    history[\"train_loss\"].append(loss.item())\n",
    "    n += 1\n",
    "    if i % ((len(train_df)//BATCH_SIZE)//10) == (len(train_df)//BATCH_SIZE)//10 - 1:\n",
    "      print(f\"epoch:{epoch+1}  index:{i+1}  train_loss:{train_loss/n:.5f}\")\n",
    "      n = 0\n",
    "      train_loss = 0\n",
    "\n",
    "  unet.eval()\n",
    "  with torch.no_grad():\n",
    "    for i, data in enumerate(val_loader):\n",
    "      inputs, labels = data[\"img\"].to(device), data[\"label\"].to(device)\n",
    "      outputs = unet(inputs)\n",
    "      loss = criterion(outputs, labels)\n",
    "      val_loss += loss.item()\n",
    "      m += 1\n",
    "      if i % (len(val_df)//BATCH_SIZE) == len(val_df)//BATCH_SIZE - 1:\n",
    "        print(f\"epoch:{epoch+1}  index:{i+1}  val_loss:{val_loss/m:.5f}\")\n",
    "        m = 0\n",
    "        val_loss = 0\n",
    "\n",
    "  scheduler.step()\n",
    "  torch.save(unet.state_dict(), f\"./train_depth8_{epoch+1}.pth\")\n",
    "  print(f\"Epoch {epoch + 1} - End\")\n",
    "print(\"finish training\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "macbookだと学習に時間がかかりすぎるので研究室にあるwindowsで行うことにした"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "future",
   "language": "python",
   "name": "future"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
