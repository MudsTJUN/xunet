{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset as BaseDataset\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional\n",
    "import albumentations as A\n",
    "import segmentation_models_pytorch as smp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2100, 22)\n",
      "Index(['Unnamed: 0', 'id', 'caseday', 'imgpath_0', 'imgpath_1', 'imgpath_2',\n",
      "       'imgpath_3', 'imgpath_4', 'imgpath_5', 'imgpath_6', 'imgpath_7',\n",
      "       'labelpath_0', 'labelpath_1', 'labelpath_2', 'labelpath_3',\n",
      "       'labelpath_4', 'labelpath_5', 'labelpath_6', 'labelpath_7', 'height',\n",
      "       'width', 'label'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "train_3D8_df = pd.read_csv(\"./data/train_3D8_df.csv\")\n",
    "val_3D8_df = pd.read_csv(\"./data/val_3D8_df.csv\")\n",
    "test_3D8_df = pd.read_csv(\"./data/test_3D8_df.csv\")\n",
    "\n",
    "train_df = train_3D8_df[train_3D8_df.label == 1].reset_index(drop=True)  #ラベル有のデータのみ抽出\n",
    "val_df = val_3D8_df[val_3D8_df.label == 1].reset_index(drop=True)  #ラベル有のデータのみ抽出\n",
    "test_df = test_3D8_df[test_3D8_df.label == 1].reset_index(drop=True)  #ラベル有のデータのみ抽出\n",
    "\n",
    "print(train_df.shape)\n",
    "print(train_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データローダーの作成\n",
    "additional_image_targets = {f\"image{i+1}\": \"image\" for i in range(8 - 1)}\n",
    "additional_label_targets = {f\"mask{i+1}\": \"mask\" for i in range(8 - 1)}\n",
    "additional_targets8 = dict(additional_image_targets, **additional_label_targets)\n",
    "\n",
    "data_transforms8 = A.Compose([\n",
    "      A.HorizontalFlip(p=0.5),\n",
    "      A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.05, rotate_limit=10, p=0.5),\n",
    "      A.OneOf([\n",
    "          A.GridDistortion(num_steps=5, distort_limit=0.05, p=1.0),\n",
    "          A.ElasticTransform(alpha=1, sigma=50, alpha_affine=50, p=1.0)\n",
    "      ], p=0.25),],\n",
    "      additional_targets = additional_targets8, p=1.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 以下のコードの説明\n",
    "1. 各深さの画像トラベルを読み込み、(256,256)にリサイズした後深さ方向に結合\n",
    "2. Albumentationsの引数とするための辞書型のデータを作成\n",
    "3. Albumentationsによるデータ拡張\n",
    "4. 変換後のデータをサイド深さ方向に結合\n",
    "5. 画像は各RGBの値を0~1に変換（255で割る）\n",
    "6. ラベルは0(background),1(large_bowel),2(small_bowel),3(stomach)で表されているため、これをOne-Hotエンコーディング\n",
    "7. 画像・ラベルデータを（バッチサイズ、チャンネル数、深さ、高さ、幅）に変換"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataset(BaseDataset):\n",
    "    def __init__(self, df):\n",
    "        self.depth = 8\n",
    "        self.imgpath_list = [[df.iloc[i,:][f\"imgpath_{h}\"] for h in range(self.depth)] for i in range(len(df))]\n",
    "        self.labelpath_list = [[df.iloc[i,:][f\"labelpath_{h}\"] for h in range(self.depth)] for i in range(len(df))]\n",
    "        self.transform8 = data_transforms8\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        imgpaths = self.imgpath_list[i]\n",
    "        labelpaths = self.labelpath_list[i]\n",
    "        for j in range(self.depth): # 画像とラベルデータの読み込み\n",
    "            img = cv2.imread(imgpaths[j])\n",
    "            img = cv2.resize(img, dsize=(256,256))\n",
    "            label = Image.open(labelpaths[j])\n",
    "            label = np.asarray(label)\n",
    "            label = cv2.resize(label,dsize=(256,256))\n",
    "            if j == 0: # 深さ方向に結合\n",
    "                img_3D = [img]\n",
    "                label_3D = [label]\n",
    "            else:\n",
    "                img_3D = np.vstack([img_3D, [img]])\n",
    "                label_3D = np.vstack([label_3D, [label]])\n",
    "\n",
    "        d1 = {\"image\": img_3D[0,:,:,:]} # Albumentationsに代入するための辞書型データを作成\n",
    "        d2 = {f\"image{i+1}\": img_3D[i+1,:,:,:] for i in range(self.depth - 1)}\n",
    "        d3 = {\"mask\": additional_label_targets[0,:,:]}\n",
    "        d4 = {f\"mask{i+1}\": label_3D[i+1,:,:] for i in range(self.depth - 1)}\n",
    "        dic = dict(d1, **d2, **d3, **d4)\n",
    "\n",
    "        transformed = self.transform8(**dic)\n",
    "\n",
    "        for j in range(self.depth):\n",
    "            if j == 0: # データ拡張後のデータを再度深さ方向に結合\n",
    "                img_3D = [transformed[\"image\"]]\n",
    "                label_3D = [transformed[\"mask\"]]\n",
    "            else:\n",
    "                img_3D = np.vstack([img_3D], [transformed[f\"image{j}\"]])\n",
    "                label_3D = np.vstack([label_3D, [transformed[f\"mask{j}\"]]])\n",
    "\n",
    "        img_3D = img_3D/255 # RGBの値を0~1に\n",
    "        img_3D = torch.from_numpy(img_3D.astype(np.float32)).clone()\n",
    "        img_3D = img_3D.permute(3, 0, 1, 2) # (チャンネル数、深さ、高さ、幅)に変換\n",
    "        label_3D = torch.from_numpy(label_3D.astype(np.float32)).clone()\n",
    "        label_3D = torch.nn.functional.one_hot(label_3D.long(), num_classes=4)\n",
    "        label_3D = label_3D.to(torch.float32)\n",
    "        label_3D = label_3D.permute(3, 0, 1, 2) # チャンネル数、深さ、高さ、幅）に変換\n",
    "        data = {\"img\": img_3D, \"label\": label_3D}\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgpath_list)\n",
    "    \n",
    "class valtest_Dataset(BaseDataset): # Albumentationsによるデータ拡張を行わない\n",
    "    def __init__(self, df):\n",
    "        self.depth = 8\n",
    "        self.imgpath_list = [[df.iloc[i,:][f\"imgpath_{h}\"] for h in range(self.depth)] for i in range(len(df))]\n",
    "        self.labelpath_list = [[df.iloc[i,:][f\"labelpath_{h}\"] for h in range(self.depth)] for i in range(len(df))]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        imgpaths = self.imgpath_list[i]\n",
    "        labelpaths = self.labelpath_list[i]\n",
    "        for j in range(self.depth):\n",
    "            img = cv2.imread(imgpaths[j])\n",
    "            label = Image.open(labelpaths[j])\n",
    "            label = np.asarray(label)\n",
    "            img = cv2.resize(img, dsize=(256,256))\n",
    "            label = cv2.resize(label, dsize=(256,256))\n",
    "            if j == 0:\n",
    "                img_3D = [img]\n",
    "                label_3D = [label]\n",
    "            else:\n",
    "                img_3D = np.vstack([img_3D, [img]])\n",
    "                label_3D = np.vstack([label_3D, [label]])\n",
    "\n",
    "        img_3D = img_3D/255\n",
    "        img_3D = torch.from_numpy(img_3D.astype(np.float32)).clone()\n",
    "        img_3D = img_3D.permute(3, 0, 1, 2)\n",
    "        label_3D = torch.from_numpy(label_3D.astype(np.float32)).clone()\n",
    "        label_3D = torch.nn.functional.one_hot(label_3D.long(), num_classes=4)\n",
    "        label_3D = label_3D.to(torch.float32)\n",
    "        label_3D = label_3D.permute(3, 0, 1, 2)\n",
    "        data = {\"img\": img_3D, \"label\": label_3D}\n",
    "        return data\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgpath_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 3\n",
    "train_dataset = Dataset(train_df)\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=BATCH_SIZE,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True)\n",
    "\n",
    "val_dataset = valtest_Dataset(val_df)\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                        batch_size=BATCH_SIZE,\n",
    "                        num_workers=4,\n",
    "                        shuffle=True)\n",
    "\n",
    "test_dataset = valtest_Dataset(test_df)\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=1,\n",
    "                         num_workers=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5DのUNetモデルを構築"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoConvBlock_2D(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, middle_channels, kernel_size=3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm2d(middle_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(middle_channels, out_channels, kernel_size=3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.rl(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2\n",
    "        x = self.rl(x)\n",
    "        return x\n",
    "    \n",
    "class TwoConvBlock_3D(nn.Module):\n",
    "    def __init__(self, in_channels, middle_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv3d(in_channels, middle_channels, kernel_size=3, padding=\"same\")\n",
    "        self.bn1 = nn.BatchNorm3d(middle_channels)\n",
    "        self.rl = nn.ReLU()\n",
    "        self.conv2 = nn.Conv3d(middle_channels, out_channels, kernel_size=3, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.rl(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.rl(x)\n",
    "        return x\n",
    "    \n",
    "class UpConv_2D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"bilinear\", align_corners=True)\n",
    "        self.bn1 = nn.BatchNorm2d(in_channels)\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, kernel_size=2, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "    \n",
    "class UpConv_3D(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.up = nn.Upsample(scale_factor=2, mode=\"trilinear\", align_corners=True)\n",
    "        self.bn1 = nn.BatchNorm3d(in_channels)\n",
    "        self.conv = nn.Conv3d(in_channels, out_channels, kernel_size=2, padding=\"same\")\n",
    "        self.bn2 = nn.BatchNorm3d(out_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.up(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv(x)\n",
    "        x = self.bn2(x)\n",
    "        return x\n",
    "    \n",
    "class UNet_3D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.TCB1 = TwoConvBlock_3D(3, 64, 64)\n",
    "        self.TCB2 = TwoConvBlock_3D(64, 128, 128)\n",
    "        self.TCB3 = TwoConvBlock_3D(128, 256, 256)\n",
    "        self.TCB4 = TwoConvBlock_2D(256, 512, 512)\n",
    "        self.TCB5 = TwoConvBlock_2D(512, 1024, 1024)\n",
    "        self.TCB6 = TwoConvBlock_2D(1024, 512, 512)\n",
    "        self.TCB7 = TwoConvBlock_3D(512, 256, 256)\n",
    "        self.TCB8 = TwoConvBlock_3D(256, 128, 128)\n",
    "        self.TCB9 = TwoConvBlock_3D(128, 64, 64)\n",
    "\n",
    "        self.maxpool_3D = nn.MaxPool3d(2, stride=2)\n",
    "        self.maxpool_2D = nn.MaxPool2d(2, stride=2)\n",
    "\n",
    "        self.UC1 = UpConv_2D(1024, 512)\n",
    "        self.UC2 = UpConv_2D(512, 256)\n",
    "        self.UC3 = UpConv_3D(256, 128)\n",
    "        self.UC4 = UpConv_3D(128, 64)\n",
    "\n",
    "        self.conv1 = nn.Conv3d(64, 4, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.TCB1(x)\n",
    "        x1 = x\n",
    "        x = self.maxpool_3D(x)\n",
    "\n",
    "        x = self.TCB2(x)\n",
    "        x2 = x\n",
    "        x = self.maxpool_3D(x)\n",
    "\n",
    "        x = self.TCB3(x)\n",
    "        x3 = x\n",
    "        x_1, x_2 = x[:,:,0,:,:], x[:,:,1,:,:]\n",
    "        x_1, x_2 = self.maxpool_2D(x_1), self.maxpool_2D(x_2)\n",
    "\n",
    "        x_1, x_2 = self.TCB4(x_1), self.TCB4(x_1)\n",
    "        x4_1, x4_2 = x_1, x_2\n",
    "        x_1, x_2 = self.maxpool_2D(x_1), self.maxpool_2D(x_2)\n",
    "\n",
    "        x_1, x_2 = self.TCB5(x_1), self.maxpool_2D(x_1)\n",
    "\n",
    "        x_1, x_2 = self.UC1(x_1), self.UC1(x_2)\n",
    "        x_1, x_2 = torch.cat([x4_1, x_1], dim=1), torch.cat([x4_2, x_2], dim=1)\n",
    "        x_1, x_2 = self.TCB6(x_1), self.TCB6(x_1)\n",
    "\n",
    "        x_1, x_2 = self.UC2(x_1), self.UC2(x_2)\n",
    "        x = torch.cat([torch.unsqueeze(x_1, 2), torch.unsqueeze(x_2, 2)], dim=2)\n",
    "        x = torch.cat([x3, x], dim=1)\n",
    "        x = self.TCB7(x)\n",
    "\n",
    "        x = self.UC3(x)\n",
    "        x = torch.cat([x2, x], dim=1)\n",
    "        x = self.TCB8(x)\n",
    "\n",
    "        x = self.UC4(x)\n",
    "        x = torch.cat([x1, x], dim=1)\n",
    "        x = self.TCB9(x)\n",
    "\n",
    "        x = self.conv1(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU, 最適化アルゴリズムの設定を行う\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "unet = UNet_3D().to(device)\n",
    "optimizer = optim.Adam(unet.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失関数の設定\n",
    "# 損失はTversky LossとBCEWithLogits Lossの平均とした。\n",
    "# これらの関数は損失関数内でソフトマックス関数を処理する為、UNetの最後にソフトマックス関数を適用しない\n",
    "TverskyLoss = smp.losses.TverskyLoss(mode='multilabel', log_loss=False)\n",
    "BCELoss = smp.losses.SoftBCEWithLogitsLoss()\n",
    "def criterion(pred, target):\n",
    "    return 0.5*BCELoss(pred, target) + 0.5*TverskyLoss(pred, target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 1, in <module>\n",
      "  File \"/Users/t-jun/.pyenv/versions/3.10.1/lib/python3.10/multiprocessing/spawn.py\", line 116, in spawn_main\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \"/Users/t-jun/.pyenv/versions/3.10.1/lib/python3.10/multiprocessing/spawn.py\", line 126, in _main\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "AttributeError: Can't get attribute 'Dataset' on <module '__main__' (built-in)>\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/t-jun/future/x-unet/u-net.ipynb セル 11\u001b[0m line \u001b[0;36m1\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/t-jun/future/x-unet/u-net.ipynb#X13sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m val_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t-jun/future/x-unet/u-net.ipynb#X13sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m unet\u001b[39m.\u001b[39mtrain()\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/t-jun/future/x-unet/u-net.ipynb#X13sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39;49m(train_loader):\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t-jun/future/x-unet/u-net.ipynb#X13sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     inputs, labels \u001b[39m=\u001b[39m data[\u001b[39m\"\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device), data[\u001b[39m\"\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/t-jun/future/x-unet/u-net.ipynb#X13sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     optimizer\u001b[39m.\u001b[39mzero_grad()\n",
      "File \u001b[0;32m~/future/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:438\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    436\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterator\n\u001b[1;32m    437\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 438\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_iterator()\n",
      "File \u001b[0;32m~/future/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:386\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    384\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    385\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 386\u001b[0m     \u001b[39mreturn\u001b[39;00m _MultiProcessingDataLoaderIter(\u001b[39mself\u001b[39;49m)\n",
      "File \u001b[0;32m~/future/.venv/lib/python3.10/site-packages/torch/utils/data/dataloader.py:1039\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1032\u001b[0m w\u001b[39m.\u001b[39mdaemon \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n\u001b[1;32m   1033\u001b[0m \u001b[39m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1034\u001b[0m \u001b[39m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1035\u001b[0m \u001b[39m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1036\u001b[0m \u001b[39m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1037\u001b[0m \u001b[39m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1038\u001b[0m \u001b[39m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1039\u001b[0m w\u001b[39m.\u001b[39;49mstart()\n\u001b[1;32m   1040\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_index_queues\u001b[39m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1041\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_workers\u001b[39m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mnot\u001b[39;00m _current_process\u001b[39m.\u001b[39m_config\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mdaemon\u001b[39m\u001b[39m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[39m'\u001b[39m\u001b[39mdaemonic processes are not allowed to have children\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_Popen(\u001b[39mself\u001b[39;49m)\n\u001b[1;32m    122\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sentinel \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_popen\u001b[39m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[39m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[39m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[39mreturn\u001b[39;00m _default_context\u001b[39m.\u001b[39;49mget_context()\u001b[39m.\u001b[39;49mProcess\u001b[39m.\u001b[39;49m_Popen(process_obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/multiprocessing/context.py:284\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    281\u001b[0m \u001b[39m@staticmethod\u001b[39m\n\u001b[1;32m    282\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    283\u001b[0m     \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mpopen_spawn_posix\u001b[39;00m \u001b[39mimport\u001b[39;00m Popen\n\u001b[0;32m--> 284\u001b[0m     \u001b[39mreturn\u001b[39;00m Popen(process_obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_fds \u001b[39m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(process_obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/multiprocessing/popen_fork.py:19\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreturncode \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinalizer \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_launch(process_obj)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.10.1/lib/python3.10/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msentinel \u001b[39m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(parent_w, \u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m, closefd\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         f\u001b[39m.\u001b[39;49mwrite(fp\u001b[39m.\u001b[39;49mgetbuffer())\n\u001b[1;32m     63\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[39m=\u001b[39m []\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 学習を行う\n",
    "history = {\"train_loss\": []}\n",
    "n = 0\n",
    "m = 0\n",
    "\n",
    "for epoch in range(15):\n",
    "    train_loss = 0\n",
    "    val_loss = 0\n",
    "\n",
    "    unet.train()\n",
    "    for i, data in enumerate(train_loader):\n",
    "        inputs, labels = data[\"img\"].to(device), data[\"label\"].to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = unet(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        history[\"train_loss\"].append(loss.item())\n",
    "        n += 1\n",
    "        if i % ((len(train_df)//BATCH_SIZE)//10) == (len(train_df)//BATCH_SIZE)//10 - 1:\n",
    "            print(f\"epoch:{epoch+1} index:{i+1} train_loss:{train_loss/n:.5f}\")\n",
    "            n = 0\n",
    "            train_loss = 0\n",
    "\n",
    "    unet.eval()\n",
    "    with torch.no_grad():\n",
    "        for i, data in enumerate(val_loader):\n",
    "            inputs, labels = data[\"img\"].to(device), data[\"label\"].to(device)\n",
    "            outputs = unet(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "            m += 1\n",
    "            if i % (len(val_df)//BATCH_SIZE) == len(val_df)//BATCH_SIZE - 1:\n",
    "                print(f\"epoch:{epoch+1} index:{i+1} val_loss:{val_loss/m:.5f}\")\n",
    "                m = 0\n",
    "                val_loss = 0\n",
    "\n",
    "    torch.save(unet.state_dict(), f\"./train_depth8_{epoch+1}.pth\")\n",
    "print(\"finish trainning\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 損失の推移をプロット。\n",
    "plt.plot(history[\"train_loss\"])\n",
    "plt.xlabel('batch')\n",
    "plt.ylabel('loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testデータに対して予測を行う\n",
    "model = UNet_3D()\n",
    "model.load_state_dict(torch.load(\"./train_\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "future",
   "language": "python",
   "name": "future"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
